import tensorflow as tfimport argparseimport osimport timedef get_options():    """ Argument parsing options"""    parser = argparse.ArgumentParser()    # == Memory time consumption ==    parser.add_argument('--batch_size', type=int, default=2, help='Batch size to use')    parser.add_argument('--steps_per_epoch', type=int, default=1000, help='Steps per epoch')    parser.add_argument('--num_epochs', type=int, default=100, help='Epochs')    parser.add_argument('--name', type=str, default='efficientdet-d0', help='EfficientNet backbone.', choices=['efficientdet-d0','efficientdet-d4'])    parser.add_argument('--evaluate',action='store_true', default=False, help='If only perform evaluation')    # parser.add_argument('--sample_perc', type=float, default=0.005, help='Percentage of pixel in contour that gets sampled, by assigning it to xtopleft,ytopleft of patch.')        # Optimizer and learning rate scheduling options    parser.add_argument('--optimizer', type=str, default='Adam', choices=['Adam', 'SGD'])    parser.add_argument('--lr_decay_method', type=str, default='cyclic', choices=['stepwise', 'cosine', 'polynomial', 'cyclic'])    parser.add_argument('--step_size', type=int, default=10, help='Step_size for the cyclic learning rate')    parser.add_argument('--gamma', type=float, default=0.99998, help='Decay parameter for the cyclic learning rate')    # == Log options ==    parser.add_argument('--log_dir', type=str, help='Folder of where the logs are saved', default='output')    parser.add_argument('--run_name', type=str, default='run')    opts = parser.parse_args()    # if not opts.evaluate:    #     opts.run_name = "{}_{}".format(opts.run_name, time.strftime("%Y%m%dT%H%M%S"))    #     opts.log_dir = os.path.join(opts.log_dir, opts.run_name)    #     os.makedirs(opts.log_dir, exist_ok=True)    opts.tf_version = f'{tf.__version__}'    return opts